from numpy import size
import torch 
import torch.nn as nn
import torch.nn.functional as F

class DiceLoss(nn.Module):
    def __init__(self, alpha=0.5, beta=0.5, size_average=True, reduce=True):
        super(DiceLoss, self).__init__()
        self.alpha = alpha
        self.beta = beta
        self.size_average = size_average
        self.reduce =reduce
    
    def forward(self, preds, targets):
        N = preds.size(0)
        C = preds.size(1)

        P = F.softmax(preds, dim=1)
        smooth = torch.zeros(C, dtype=torch.float32).fill_(0.00001) # create a matrix with every elements set to be 0.00001 

        class_mask = torch.zeros(preds.shape).to(preds.device)
        class_mask.scatter_(1, targets, 1.) # scatter_函数将src中数据根据index中的索引按照dim=1(行)的方向填进class_mask中, one-hot表示

        ones = torch.ones(preds.shape).to(preds.device)

        P_ = ones - P
        class_mask_ = ones - class_mask

        TP = P * class_mask
        FP = P * class_mask_
        FN = P_ * class_mask

        smooth = smooth.to(preds.device)
        self.alpha = FP.transpose(0, 1).reshape(C, -1).sum(dim=(1)) / ((FP.transpose(0, 1).reshape(C, -1).sum(dim=(1)) + FN.transpose(0, 1).reshape(C, -1).sum(dim=(1))) + smooth)

        self.alpha = torch.clamp(self.alpha, min=0.2, max=0.8)
        self.beta = 1 - self.alpha
        num = torch.sum(TP.transpose(0, 1).reshape(C, -1), dim=(1)).float()
        den = num + self.alpha * torch.sum(FP.transpose(0, 1).reshape(C, -1), dim=(1)).float() + self.beta * torch.sum(FN.transpose(0, 1).reshape(C, -1), dim=(1)).float()

        dice = num / (den + smooth)
        if not self.reduce:
            loss = torch.ones(C).to(dice.device) - dice
            return loss
        
        loss = 1 - dice
        loss = loss.sum()

        if self.size_average:
            loss /= C
        
        return loss
